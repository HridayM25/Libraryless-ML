{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Have used the sklearn library only for the iris dataset, no further use of it will be done.**"
      ],
      "metadata": {
        "id": "MD8iVymJbi5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "iris=datasets.load_iris()\n",
        "X=iris.data\n",
        "y=iris.target"
      ],
      "metadata": {
        "id": "FcGYlYQEXSpC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The only library we will be using for the entirety of the task."
      ],
      "metadata": {
        "id": "DpN5GpwZbrf9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "c_i8lCz8eCwK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The randon_gen() function will generate a random number (integer), which will either be added or subtracted based on a condition."
      ],
      "metadata": {
        "id": "8ckCaftFb0y3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def random_gen():\n",
        "  current=time.time()\n",
        "  last_digit=int(str(current-int(current))[-1])\n",
        "  if last_digit%2==0:\n",
        "    return last_digit\n",
        "  else:\n",
        "    return -1*last_digit"
      ],
      "metadata": {
        "id": "TtM1DrE-eHng"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to convert the data and the targets to lists since they are originally ndarrays, and we will not be using the numpy library anywhere"
      ],
      "metadata": {
        "id": "wpYbQ0ZTcEVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert both variables to list since originally they are ndarrays\n",
        "X=X.tolist()\n",
        "y=y.tolist()"
      ],
      "metadata": {
        "id": "myvWNLSoXbcc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Helper Functions defined here:\n",
        "\n",
        "#Function for natural log, later can be used for log base 10. Can also try the Taylor Definition, but this seemed easier. I will compare precision later.\n",
        "def ln(x):\n",
        "    n = 10000000.0\n",
        "    return n * ((x ** (1/n)) - 1)\n",
        "\n",
        "#Can convert to base 10\n",
        "def log(x):\n",
        "  return ln(x)/ln(10)\n",
        "\n",
        "#Function to calculate derivative of a function\n",
        "def derivative(value,function):\n",
        "  step=0.000001\n",
        "  return (function(value+step) - function(value-step))/(2*step)\n",
        "\n",
        "#Function to calculate the sigmoid\n",
        "e=2.71828\n",
        "def sigmoid(value):\n",
        "  return 1/(1+e**(-1*value))\n",
        "\n",
        "#Function to calculate the derivative of Binary Cross Entropy Loss\n",
        "def bce_loss_derivative(y_true,z):\n",
        "  return (y_true-z)/(z)*(1-z)\n",
        "\n",
        "#Function to calculate the derivative of the sigmoid function\n",
        "def sigmoid_derivative(value):\n",
        "  return sigmoid(value) * (1-sigmoid(value))\n",
        "\n",
        "#Function to calculate the Binary Cross Entropy Loss\n",
        "def bce_loss(value,y_true):\n",
        "  return -1*((y_true * log(value)) + ((1-y_true) * log(1-value)))\n",
        "\n",
        "#Function to calculate the softmax\n",
        "def softmax(layer):\n",
        "  sum=0\n",
        "  for i in range(len(layer)):\n",
        "    sum+= 2.71828**layer[i]\n",
        "  for i in range(len(layer)):\n",
        "    layer[i]=2.71828**layer[i]/sum\n",
        "\n",
        "#Function to calculate categorical loss\n",
        "def categorical_loss(layer,y_true):\n",
        "  loss=log(layer[y_true])\n",
        "  return loss\n",
        "\n",
        "#Function to calculate derivative of categorical loss\n",
        "def cl_derivative(value,y_true):\n",
        "  one_hot=[0,0,0]\n",
        "  one_hot[y_true]=1\n",
        "  return value-one_hot[0]\n",
        "\n",
        "#Function to take deepcopy of an object\n",
        "def deep_copy(obj):\n",
        "    if isinstance(obj, list):\n",
        "        return [deep_copy(item) for item in obj]\n",
        "    elif isinstance(obj, dict):\n",
        "        return {key: deep_copy(value) for key, value in obj.items()}\n",
        "    else:\n",
        "        return obj"
      ],
      "metadata": {
        "id": "hs_DsIfPggMr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is just to see what kind of classification problem is there, and the number of training examples, and features."
      ],
      "metadata": {
        "id": "HlnzjkVwcQN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The number of training examples are : \" + str(len(X)))\n",
        "print(\"The number of attributes/features are : \" + str(len(X[0])))\n",
        "print(\"The number of unique classes are : \" + str(len(set(y))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zh1IyoIdYx0l",
        "outputId": "003588e1-26b1-477c-bb66-6039fb9f18f5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of training examples are : 150\n",
            "The number of attributes/features are : 4\n",
            "The number of unique classes are : 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since there are 3 classes, this becomes a multi-class classification problem.\n",
        "Hence, we can structure our architecture in such a way that the final layer has 3 neurons, over which we can apply softmax, and then further find the loss using categorical loss."
      ],
      "metadata": {
        "id": "M3tU3E6VcciD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5cvB7W3YLPq",
        "outputId": "5cef0b95-ab60-44e5-817a-782dbd25de94"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[5.1, 3.5, 1.4, 0.2],\n",
              " [4.9, 3.0, 1.4, 0.2],\n",
              " [4.7, 3.2, 1.3, 0.2],\n",
              " [4.6, 3.1, 1.5, 0.2],\n",
              " [5.0, 3.6, 1.4, 0.2],\n",
              " [5.4, 3.9, 1.7, 0.4],\n",
              " [4.6, 3.4, 1.4, 0.3],\n",
              " [5.0, 3.4, 1.5, 0.2],\n",
              " [4.4, 2.9, 1.4, 0.2],\n",
              " [4.9, 3.1, 1.5, 0.1],\n",
              " [5.4, 3.7, 1.5, 0.2],\n",
              " [4.8, 3.4, 1.6, 0.2],\n",
              " [4.8, 3.0, 1.4, 0.1],\n",
              " [4.3, 3.0, 1.1, 0.1],\n",
              " [5.8, 4.0, 1.2, 0.2],\n",
              " [5.7, 4.4, 1.5, 0.4],\n",
              " [5.4, 3.9, 1.3, 0.4],\n",
              " [5.1, 3.5, 1.4, 0.3],\n",
              " [5.7, 3.8, 1.7, 0.3],\n",
              " [5.1, 3.8, 1.5, 0.3],\n",
              " [5.4, 3.4, 1.7, 0.2],\n",
              " [5.1, 3.7, 1.5, 0.4],\n",
              " [4.6, 3.6, 1.0, 0.2],\n",
              " [5.1, 3.3, 1.7, 0.5],\n",
              " [4.8, 3.4, 1.9, 0.2],\n",
              " [5.0, 3.0, 1.6, 0.2],\n",
              " [5.0, 3.4, 1.6, 0.4],\n",
              " [5.2, 3.5, 1.5, 0.2],\n",
              " [5.2, 3.4, 1.4, 0.2],\n",
              " [4.7, 3.2, 1.6, 0.2],\n",
              " [4.8, 3.1, 1.6, 0.2],\n",
              " [5.4, 3.4, 1.5, 0.4],\n",
              " [5.2, 4.1, 1.5, 0.1],\n",
              " [5.5, 4.2, 1.4, 0.2],\n",
              " [4.9, 3.1, 1.5, 0.2],\n",
              " [5.0, 3.2, 1.2, 0.2],\n",
              " [5.5, 3.5, 1.3, 0.2],\n",
              " [4.9, 3.6, 1.4, 0.1],\n",
              " [4.4, 3.0, 1.3, 0.2],\n",
              " [5.1, 3.4, 1.5, 0.2],\n",
              " [5.0, 3.5, 1.3, 0.3],\n",
              " [4.5, 2.3, 1.3, 0.3],\n",
              " [4.4, 3.2, 1.3, 0.2],\n",
              " [5.0, 3.5, 1.6, 0.6],\n",
              " [5.1, 3.8, 1.9, 0.4],\n",
              " [4.8, 3.0, 1.4, 0.3],\n",
              " [5.1, 3.8, 1.6, 0.2],\n",
              " [4.6, 3.2, 1.4, 0.2],\n",
              " [5.3, 3.7, 1.5, 0.2],\n",
              " [5.0, 3.3, 1.4, 0.2],\n",
              " [7.0, 3.2, 4.7, 1.4],\n",
              " [6.4, 3.2, 4.5, 1.5],\n",
              " [6.9, 3.1, 4.9, 1.5],\n",
              " [5.5, 2.3, 4.0, 1.3],\n",
              " [6.5, 2.8, 4.6, 1.5],\n",
              " [5.7, 2.8, 4.5, 1.3],\n",
              " [6.3, 3.3, 4.7, 1.6],\n",
              " [4.9, 2.4, 3.3, 1.0],\n",
              " [6.6, 2.9, 4.6, 1.3],\n",
              " [5.2, 2.7, 3.9, 1.4],\n",
              " [5.0, 2.0, 3.5, 1.0],\n",
              " [5.9, 3.0, 4.2, 1.5],\n",
              " [6.0, 2.2, 4.0, 1.0],\n",
              " [6.1, 2.9, 4.7, 1.4],\n",
              " [5.6, 2.9, 3.6, 1.3],\n",
              " [6.7, 3.1, 4.4, 1.4],\n",
              " [5.6, 3.0, 4.5, 1.5],\n",
              " [5.8, 2.7, 4.1, 1.0],\n",
              " [6.2, 2.2, 4.5, 1.5],\n",
              " [5.6, 2.5, 3.9, 1.1],\n",
              " [5.9, 3.2, 4.8, 1.8],\n",
              " [6.1, 2.8, 4.0, 1.3],\n",
              " [6.3, 2.5, 4.9, 1.5],\n",
              " [6.1, 2.8, 4.7, 1.2],\n",
              " [6.4, 2.9, 4.3, 1.3],\n",
              " [6.6, 3.0, 4.4, 1.4],\n",
              " [6.8, 2.8, 4.8, 1.4],\n",
              " [6.7, 3.0, 5.0, 1.7],\n",
              " [6.0, 2.9, 4.5, 1.5],\n",
              " [5.7, 2.6, 3.5, 1.0],\n",
              " [5.5, 2.4, 3.8, 1.1],\n",
              " [5.5, 2.4, 3.7, 1.0],\n",
              " [5.8, 2.7, 3.9, 1.2],\n",
              " [6.0, 2.7, 5.1, 1.6],\n",
              " [5.4, 3.0, 4.5, 1.5],\n",
              " [6.0, 3.4, 4.5, 1.6],\n",
              " [6.7, 3.1, 4.7, 1.5],\n",
              " [6.3, 2.3, 4.4, 1.3],\n",
              " [5.6, 3.0, 4.1, 1.3],\n",
              " [5.5, 2.5, 4.0, 1.3],\n",
              " [5.5, 2.6, 4.4, 1.2],\n",
              " [6.1, 3.0, 4.6, 1.4],\n",
              " [5.8, 2.6, 4.0, 1.2],\n",
              " [5.0, 2.3, 3.3, 1.0],\n",
              " [5.6, 2.7, 4.2, 1.3],\n",
              " [5.7, 3.0, 4.2, 1.2],\n",
              " [5.7, 2.9, 4.2, 1.3],\n",
              " [6.2, 2.9, 4.3, 1.3],\n",
              " [5.1, 2.5, 3.0, 1.1],\n",
              " [5.7, 2.8, 4.1, 1.3],\n",
              " [6.3, 3.3, 6.0, 2.5],\n",
              " [5.8, 2.7, 5.1, 1.9],\n",
              " [7.1, 3.0, 5.9, 2.1],\n",
              " [6.3, 2.9, 5.6, 1.8],\n",
              " [6.5, 3.0, 5.8, 2.2],\n",
              " [7.6, 3.0, 6.6, 2.1],\n",
              " [4.9, 2.5, 4.5, 1.7],\n",
              " [7.3, 2.9, 6.3, 1.8],\n",
              " [6.7, 2.5, 5.8, 1.8],\n",
              " [7.2, 3.6, 6.1, 2.5],\n",
              " [6.5, 3.2, 5.1, 2.0],\n",
              " [6.4, 2.7, 5.3, 1.9],\n",
              " [6.8, 3.0, 5.5, 2.1],\n",
              " [5.7, 2.5, 5.0, 2.0],\n",
              " [5.8, 2.8, 5.1, 2.4],\n",
              " [6.4, 3.2, 5.3, 2.3],\n",
              " [6.5, 3.0, 5.5, 1.8],\n",
              " [7.7, 3.8, 6.7, 2.2],\n",
              " [7.7, 2.6, 6.9, 2.3],\n",
              " [6.0, 2.2, 5.0, 1.5],\n",
              " [6.9, 3.2, 5.7, 2.3],\n",
              " [5.6, 2.8, 4.9, 2.0],\n",
              " [7.7, 2.8, 6.7, 2.0],\n",
              " [6.3, 2.7, 4.9, 1.8],\n",
              " [6.7, 3.3, 5.7, 2.1],\n",
              " [7.2, 3.2, 6.0, 1.8],\n",
              " [6.2, 2.8, 4.8, 1.8],\n",
              " [6.1, 3.0, 4.9, 1.8],\n",
              " [6.4, 2.8, 5.6, 2.1],\n",
              " [7.2, 3.0, 5.8, 1.6],\n",
              " [7.4, 2.8, 6.1, 1.9],\n",
              " [7.9, 3.8, 6.4, 2.0],\n",
              " [6.4, 2.8, 5.6, 2.2],\n",
              " [6.3, 2.8, 5.1, 1.5],\n",
              " [6.1, 2.6, 5.6, 1.4],\n",
              " [7.7, 3.0, 6.1, 2.3],\n",
              " [6.3, 3.4, 5.6, 2.4],\n",
              " [6.4, 3.1, 5.5, 1.8],\n",
              " [6.0, 3.0, 4.8, 1.8],\n",
              " [6.9, 3.1, 5.4, 2.1],\n",
              " [6.7, 3.1, 5.6, 2.4],\n",
              " [6.9, 3.1, 5.1, 2.3],\n",
              " [5.8, 2.7, 5.1, 1.9],\n",
              " [6.8, 3.2, 5.9, 2.3],\n",
              " [6.7, 3.3, 5.7, 2.5],\n",
              " [6.7, 3.0, 5.2, 2.3],\n",
              " [6.3, 2.5, 5.0, 1.9],\n",
              " [6.5, 3.0, 5.2, 2.0],\n",
              " [6.2, 3.4, 5.4, 2.3],\n",
              " [5.9, 3.0, 5.1, 1.8]]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Choose architecture here:\n",
        "num_layers = 5\n",
        "#That implies 1 input layer, 4 hidden layers, and 1 output layer\n",
        "no_of_neurons=[4,5,4,3,4]\n",
        "final_layer=(len(set(y)))\n",
        "no_of_neurons.append(final_layer)\n",
        "n=no_of_neurons"
      ],
      "metadata": {
        "id": "9m1jLSMlYMw3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIPWrPH-pS5N",
        "outputId": "6be3e2cc-e8ab-4059-fa21-517f0bf78cd6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4, 5, 4, 3, 4, 3]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is our architecture. The last layer as mentioned is the number of classes to help us find the softmax."
      ],
      "metadata": {
        "id": "l6vcLfl4c7cv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#define the weights here:\n",
        "Weights=[]\n",
        "for i in range(len(n)-1):\n",
        "  current=time.time()\n",
        "  m=int(str(current-int(current))[-1])\n",
        "  Weights.append([[(m/10 + random_gen())/10 for x in range(n[i+1])] for y in range(n[i])])"
      ],
      "metadata": {
        "id": "FoEb5IRwaMwz"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define the biases here:\n",
        "Biases=[]\n",
        "for i in range(len(n)-1):\n",
        "  current=time.time()\n",
        "  m=int(str(current-int(current))[-1])\n",
        "  Biases.append([(m/10+ random_gen())/10 for x in range(n[i+1])])"
      ],
      "metadata": {
        "id": "MhoFk4usblUO"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define the gradients of weights here:\n",
        "Weights_Grad=[]\n",
        "for i in range(len(n)-1):\n",
        "  Weights_Grad.append([[0 for x in range(n[i+1])] for y in range(n[i])])"
      ],
      "metadata": {
        "id": "Hig6reaWh4dy"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define the gradients of biases here:\n",
        "Biases_Grad=[]\n",
        "for i in range(len(n)-1):\n",
        "  Biases_Grad.append([0 for x in range(n[i+1])])"
      ],
      "metadata": {
        "id": "KTtjY7_unPyQ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define the activations here\n",
        "Activations=[]\n",
        "for i in range(len(n)-1):\n",
        "  Activations.append([0 for x in range(n[i+1])])"
      ],
      "metadata": {
        "id": "GwXJzIFBiv1e"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define cached activations here\n",
        "ad=[]\n",
        "for i in range(len(n)-1):\n",
        "  ad.append([0 for x in range(n[i+1])])\n"
      ],
      "metadata": {
        "id": "h2AXBTlFZW5p"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the weights, biases here.\n",
        "The gradients of weights as well as the activations are initialized to 0."
      ],
      "metadata": {
        "id": "cMRNDPpwdfUL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uedi1p6CgRFF",
        "outputId": "8c6f7a3f-1363-4ab0-cff4-07ef0a62baee"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[[0.89, -0.009999999999999998, 0.6900000000000001, -0.41, -0.41],\n",
              "  [0.29,\n",
              "   -0.009999999999999998,\n",
              "   -0.8099999999999999,\n",
              "   -0.41,\n",
              "   0.6900000000000001],\n",
              "  [0.29, -0.009999999999999998, 0.29, -0.009999999999999998, 0.89],\n",
              "  [-0.8099999999999999, 0.89, -0.61, -0.8099999999999999, 0.89]],\n",
              " [[-0.61, -0.61, -0.61, 0.6900000000000001],\n",
              "  [0.6900000000000001, -0.8099999999999999, 0.89, -0.61],\n",
              "  [-0.8099999999999999, -0.8099999999999999, 0.89, -0.61],\n",
              "  [-0.8099999999999999, 0.6900000000000001, 0.6900000000000001, -0.41],\n",
              "  [0.89, 0.6900000000000001, 0.6900000000000001, 0.6900000000000001]],\n",
              " [[0.47000000000000003, -0.43, 0.47000000000000003],\n",
              "  [0.8699999999999999, -0.8300000000000001, -0.22999999999999998],\n",
              "  [-0.63, 0.27, 0.8699999999999999],\n",
              "  [0.47000000000000003, 0.8699999999999999, 0.8699999999999999]],\n",
              " [[-0.63, -0.8300000000000001, 0.8699999999999999, -0.8300000000000001],\n",
              "  [0.27, -0.030000000000000006, 0.67, 0.27],\n",
              "  [0.67, 0.8699999999999999, -0.63, 0.47000000000000003]],\n",
              " [[-0.6799999999999999, -0.08, 0.22000000000000003],\n",
              "  [-0.48, -0.48, -0.48],\n",
              "  [-0.6799999999999999, 0.62, 0.62],\n",
              "  [0.82, 0.62, 0.82]]]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Biases"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwuX7v0ngXaX",
        "outputId": "59e9ba18-5664-49ba-d496-2b46fa1a5fd4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[-0.08, 0.22000000000000003, 0.42000000000000004, -0.6799999999999999, 0.82],\n",
              " [-0.6900000000000001, -0.6900000000000001, -0.49000000000000005, -0.29],\n",
              " [0.43, 0.22999999999999998, 0.43],\n",
              " [-0.47000000000000003, -0.27, -0.27, -0.06999999999999999],\n",
              " [-0.24, -0.04, -0.04]]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Weights_Grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzoBzqRiiAnu",
        "outputId": "91fd226f-6608-4e31-c05b-99e963abfe3b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[[0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0]],\n",
              " [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
              " [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
              " [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
              " [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]]]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Biases_Grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_AnorWpoATm",
        "outputId": "23418cfb-76a3-4732-96e9-bfd454905c29"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0, 0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0, 0, 0]]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Activations"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rcM23VNi-Ad",
        "outputId": "87b5215b-86e2-490b-a175-ef99d0a0f72e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0, 0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0, 0, 0]]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w=Weights\n",
        "b=Biases\n",
        "w_grad=Weights_Grad\n",
        "b_grad=Biases_Grad\n",
        "a=Activations\n",
        "#ca=Cached_Activations\n",
        "w_original=deep_copy(w)\n",
        "#b_original=deep_copy(b)"
      ],
      "metadata": {
        "id": "ucAEGdmXjHfS"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the dataset into train and test, into a 7:3 ratio. Hence 105 for training and 45 for testing.\n",
        "\n",
        "Also check the number of instances of each class, to see if any methods need to be applied to mitigate the effects of class imbalance.\n",
        "\n",
        "Here, advantage is taken of the fact that all classes occur in equal number, and hence slicing values are chosen in such a way to ensure we have an equal number of instances of each class in the training and testing data."
      ],
      "metadata": {
        "id": "xHRRm6WpdvyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "indices = [slice(0, 35), slice(50, 85), slice(100, 135)]\n",
        "test_indices=[slice(35,50), slice(85,100), slice(135,150)]\n",
        "X_train = []\n",
        "y_train=[]\n",
        "X_test=[]\n",
        "y_test=[]\n",
        "\n",
        "for id in indices:\n",
        "    X_train += X[id]\n",
        "    y_train += y[id]\n",
        "for id in test_indices:\n",
        "  X_test+= X[id]\n",
        "  y_test+= y[id]"
      ],
      "metadata": {
        "id": "dlBaI4DbjUAp"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_train), len(X_test)\n",
        "#Checks out!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_bEPZYjoG2P",
        "outputId": "6cc9c713-2f0b-4778-b5b3-dcf24afbbe46"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(105, 45)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count0=y.count(0)\n",
        "count1=y.count(1)\n",
        "count2=y.count(2)"
      ],
      "metadata": {
        "id": "NU1i56K2jpia"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count0,count1,count2\n",
        "#(0-49) , (50,99) , (100,149)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOHYxTdpkMMc",
        "outputId": "57cd2c04-19b9-4be2-e5c0-a065dda412d2"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50, 50, 50)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ca=[]"
      ],
      "metadata": {
        "id": "-ntVE9JuZ3pY"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_pass(message,true_label,X):\n",
        "  input_layer=X[message]\n",
        "  for i in range(len(n)-2):\n",
        "    for j in range(n[i+1]):\n",
        "      for k in range(n[i]):\n",
        "        a[i][j]+=w[i][k][j] * input_layer[k]\n",
        "      a[i][j] += b[i][j]\n",
        "      a[i][j] = sigmoid(a[i][j])\n",
        "    input_layer = a[i]\n",
        "  softmax_layer=a[-2]\n",
        "  i=len(n)-2\n",
        "  for j in range(n[i+1]):\n",
        "    for k in range(n[i]):\n",
        "      a[i][j]+=w[i][k][j] * input_layer[k]\n",
        "    a[i][j]+=b[i][j]\n",
        "  output_layer=a[-1]\n",
        "  softmax(output_layer)\n",
        "  t=deep_copy(a)\n",
        "  loss=categorical_loss(output_layer,true_label)\n",
        "  for xx in range(len(a)):\n",
        "    for yy in range(len(a[xx])):\n",
        "      a[xx][yy]=0\n",
        "  return (loss,t)\n",
        "\n",
        "def get_loss(X_train,y_train):\n",
        "  net_loss=0\n",
        "  ca=[]\n",
        "  for message in range(len(X_train)):\n",
        "    loss,t=forward_pass(message,y_train[message],X_train)\n",
        "    net_loss+=loss\n",
        "    ca.append(t)\n",
        "\n",
        "  return (net_loss,ca)"
      ],
      "metadata": {
        "id": "cQQihKjw63pA"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l,cache=get_loss(X_train,y_train)"
      ],
      "metadata": {
        "id": "XyFggqSImahc"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the hyperparamter learning rate as 0.01 here."
      ],
      "metadata": {
        "id": "inWRh4yCoIYR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate=0.01"
      ],
      "metadata": {
        "id": "D5PUpdvSlgh6"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Backpropogation starts here!"
      ],
      "metadata": {
        "id": "ZP1Khfcd5IV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Lets just update the last layer first\n",
        "for number_cache in range(len(cache[0][-1])):\n",
        "  for number in range(len(w_grad[-1])):\n",
        "    grad_w=0\n",
        "    for message in range(len(X_train)):\n",
        "      grad_w+=(cl_derivative(cache[message][-1][number_cache], y_train[message]) * cache[message][-2][number])\n",
        "    grad_w=grad_w/len(X_train)\n",
        "    w_grad[-1][number][number_cache]=grad_w\n",
        "    w[-1][number][number_cache]-=(learning_rate*grad_w)"
      ],
      "metadata": {
        "id": "j1XvMHM1pEmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for number_cache in range(len(cache[0][-1])):\n",
        "  grad_b=0\n",
        "  grad_ad=0\n",
        "  for message in range(len(X_train)):\n",
        "    grad_b+=(cl_derivative(cache[message][-1][number_cache], y_train[message]))\n",
        "  grad_b=grad_b/len(X_train)\n",
        "  b_grad[-1][number_cache]=grad_b\n",
        "  b[-1][number_cache]-=(learning_rate*grad_b)"
      ],
      "metadata": {
        "id": "oKpjaZKYpok0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check the last cell for the updates in the weights and biases\n",
        "print(w[-1], b[-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YB-m2PDNlqUM",
        "outputId": "87c64643-31c1-4eb5-edec-75b4bcf55069"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.2598851785525077, 0.6606487894703201, -0.040526935491449666], [0.6597715120161242, -0.2387858162662547, 0.6589929129208668], [-0.44015840654170535, 0.4608165418373473, -0.840684579057462], [0.45989280863681486, -0.6393769824630723, -0.04050127852973829]] [-0.07039232236437923, -0.8678354113423553, 0.6282277337067347]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"layer=-2\n",
        "while(layer>=(-1*(n-1))):\n",
        "  for number_cache in range(len(cache[0][-1])):\n",
        "    for number in range(len(w_grad[-1])):\n",
        "      grad_w=0\n",
        "      for message in range(len(X_train)):\n",
        "        grad_w+=(cl_derivative(cache[message][layer][number_cache], y_train[message]) * cache[message][layer-1][number])\n",
        "      grad_w=grad_w/len(X_train)\n",
        "      w_grad[-1][number][number_cache]=grad_w\n",
        "      w[-1][number][number_cache]-=(learning_rate*grad_w)\"\"\""
      ],
      "metadata": {
        "id": "KCEc3cAqxdav"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}