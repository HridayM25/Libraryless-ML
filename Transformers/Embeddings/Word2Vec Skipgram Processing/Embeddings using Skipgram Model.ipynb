{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e6b3c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2d8abed",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_sentences = \"\"\"This paper reviews the artificial intelligent algorithms in engine management. This study\n",
    "provides a clear image of the current state of affairs for the past 15 years and provides fresh insights\n",
    "and improvements for future directions in the field of engine management. The scope of this paper\n",
    "comprises three main aspects to be discussed, namely, engine performance, engine control, and engine\n",
    "diagnosis. The first is associated with the need to control the basic characteristics that prove that the\n",
    "engine is working properly, namely, emission control and fuel economy.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf5a87cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_gen():\n",
    "    \"\"\"\n",
    "    Function to get a random values for the weights and the biases\n",
    "    \n",
    "    Returns: Random Integer\n",
    "    \n",
    "    \"\"\"\n",
    "    current=time.time()\n",
    "    last_digit=int(str(current-int(current))[-1])\n",
    "    if last_digit%2==0:\n",
    "        return last_digit\n",
    "    else:\n",
    "        return -1*last_digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e0308de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tags(corpus):\n",
    "    \"\"\"\n",
    "    Args : List of words\n",
    "    \n",
    "    Cleans the words of any tags and special characters.\n",
    "    \n",
    "    \"\"\"\n",
    "    for number in range(len(corpus)):\n",
    "        corpus[number] = corpus[number].replace(\"\\n\", \" \")\n",
    "        corpus[number] = corpus[number].replace(\".\", \"\")\n",
    "        corpus[number] = corpus[number].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c35e9560",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(words_list):\n",
    "    \"\"\"\n",
    "    Creates a vocabulary list that can be used for one hot encoding later.\n",
    "    \n",
    "    Args : List of sorted words\n",
    "    \n",
    "    Returns : Vocabulary List\n",
    "    \n",
    "    \"\"\"\n",
    "    vocab={}\n",
    "    index=0\n",
    "    for _ ,word in enumerate(words):\n",
    "        if word in vocab.keys():\n",
    "            continue\n",
    "        else:\n",
    "            vocab[word]=index\n",
    "            index+=1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6b439cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = create_vocabulary(english_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9ec14b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(word, vocabulary=vocab):\n",
    "    \"\"\"\n",
    "    Returns the one hot encoding of the word with respect to the vocabulary\n",
    "    \n",
    "    Args : specific word to encode, vocabulary\n",
    "    \n",
    "    Returns : one hot encoding of the word\n",
    "    \n",
    "    \"\"\"\n",
    "    one_hot_list=[0 for x in range(len(vocabulary))]\n",
    "    one_hot_list[vocabulary[word]]=1\n",
    "    return one_hot_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7478835e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_matrix(dimensions, vocabulary = vocab):\n",
    "    \"\"\"\n",
    "    Create the feature matrix which is used to make the embedding matrix\n",
    "    \n",
    "    Args : Dimensions of the embeddings specified, Vocabulary used.\n",
    "    \n",
    "    Returns : Feature Matrix\n",
    "    \n",
    "    \"\"\"\n",
    "    feature_matrix = [[(random_gen() + ((y*y/100)*random_gen()/10 + random_gen()) + x*random_gen())/100 for y in range(len(vocabulary))] for x in range(dimensions)]\n",
    "    return feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b437aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(word, feature_matrix=E):\n",
    "    \"\"\"\n",
    "    Create the embedding matrix for a word\n",
    "    \n",
    "    Args : Word, Feature Matrix \n",
    "    \n",
    "    Returns : Embedding Matrix\n",
    "    \n",
    "    \"\"\"\n",
    "    embedding_matrix=[]\n",
    "    one_hot_list=one_hot_encoding(word)\n",
    "    for i in range(len(feature_matrix)):\n",
    "        summ=0\n",
    "        for j in range(len(feature_matrix[0])):\n",
    "            summ+=one_hot_list[j] * feature_matrix[i][j]\n",
    "        embedding_matrix.append(summ)\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a680800a",
   "metadata": {},
   "outputs": [],
   "source": [
    "words= english_sentences.split()\n",
    "remove_tags(words)\n",
    "words.sort()\n",
    "vocab = create_vocabulary(words)\n",
    "E = create_feature_matrix(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5231a758",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make context, target pairs (window of only 1, in the forward direction only)\n",
    "dataset=[]\n",
    "sentences =english_sentences.split()\n",
    "remove_tags(sentences)\n",
    "for index in range(len(sentences)-1):\n",
    "    dataset.append((sentences[index], sentences[index+1]))\n",
    "    #dataset.append((sentences[index], sentences[index+2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "30a6d821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('this', 'paper'),\n",
       " ('paper', 'reviews'),\n",
       " ('reviews', 'the'),\n",
       " ('the', 'artificial'),\n",
       " ('artificial', 'intelligent'),\n",
       " ('intelligent', 'algorithms'),\n",
       " ('algorithms', 'in'),\n",
       " ('in', 'engine'),\n",
       " ('engine', 'management'),\n",
       " ('management', 'this'),\n",
       " ('this', 'study'),\n",
       " ('study', 'provides'),\n",
       " ('provides', 'a'),\n",
       " ('a', 'clear'),\n",
       " ('clear', 'image'),\n",
       " ('image', 'of'),\n",
       " ('of', 'the'),\n",
       " ('the', 'current'),\n",
       " ('current', 'state'),\n",
       " ('state', 'of'),\n",
       " ('of', 'affairs'),\n",
       " ('affairs', 'for'),\n",
       " ('for', 'the'),\n",
       " ('the', 'past'),\n",
       " ('past', '15'),\n",
       " ('15', 'years'),\n",
       " ('years', 'and'),\n",
       " ('and', 'provides'),\n",
       " ('provides', 'fresh'),\n",
       " ('fresh', 'insights'),\n",
       " ('insights', 'and'),\n",
       " ('and', 'improvements'),\n",
       " ('improvements', 'for'),\n",
       " ('for', 'future'),\n",
       " ('future', 'directions'),\n",
       " ('directions', 'in'),\n",
       " ('in', 'the'),\n",
       " ('the', 'field'),\n",
       " ('field', 'of'),\n",
       " ('of', 'engine'),\n",
       " ('engine', 'management'),\n",
       " ('management', 'the'),\n",
       " ('the', 'scope'),\n",
       " ('scope', 'of'),\n",
       " ('of', 'this'),\n",
       " ('this', 'paper'),\n",
       " ('paper', 'comprises'),\n",
       " ('comprises', 'three'),\n",
       " ('three', 'main'),\n",
       " ('main', 'aspects'),\n",
       " ('aspects', 'to'),\n",
       " ('to', 'be'),\n",
       " ('be', 'discussed,'),\n",
       " ('discussed,', 'namely,'),\n",
       " ('namely,', 'engine'),\n",
       " ('engine', 'performance,'),\n",
       " ('performance,', 'engine'),\n",
       " ('engine', 'control,'),\n",
       " ('control,', 'and'),\n",
       " ('and', 'engine'),\n",
       " ('engine', 'diagnosis'),\n",
       " ('diagnosis', 'the'),\n",
       " ('the', 'first'),\n",
       " ('first', 'is'),\n",
       " ('is', 'associated'),\n",
       " ('associated', 'with'),\n",
       " ('with', 'the'),\n",
       " ('the', 'need'),\n",
       " ('need', 'to'),\n",
       " ('to', 'control'),\n",
       " ('control', 'the'),\n",
       " ('the', 'basic'),\n",
       " ('basic', 'characteristics'),\n",
       " ('characteristics', 'that'),\n",
       " ('that', 'prove'),\n",
       " ('prove', 'that'),\n",
       " ('that', 'the'),\n",
       " ('the', 'engine'),\n",
       " ('engine', 'is'),\n",
       " ('is', 'working'),\n",
       " ('working', 'properly,'),\n",
       " ('properly,', 'namely,'),\n",
       " ('namely,', 'emission'),\n",
       " ('emission', 'control'),\n",
       " ('control', 'and'),\n",
       " ('and', 'fuel'),\n",
       " ('fuel', 'economy')]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6427c37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset[i][0]--->context\n",
    "#dataset[i][1]---->target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4f82790f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper Functions defined here:\n",
    "\n",
    "#Function for natural log, later can be used for log base 10. Can also try the Taylor Definition, but this seemed easier. I will compare precision later.\n",
    "def ln(x):\n",
    "    n = 10000000.0\n",
    "    return n * ((x ** (1/n)) - 1)\n",
    "\n",
    "#Can convert to base 10\n",
    "def log(x):\n",
    "  return ln(x)/ln(10)\n",
    "\n",
    "#Function to calculate derivative of a function\n",
    "def derivative(value,function):\n",
    "  step=0.000001\n",
    "  return (function(value+step) - function(value-step))/(2*step)\n",
    "\n",
    "#Function to calculate the sigmoid\n",
    "e=2.71828\n",
    "def sigmoid(value):\n",
    "  return 1/(1+e**(-1*value))\n",
    "\n",
    "#Function to calculate the derivative of Binary Cross Entropy Loss\n",
    "def bce_loss_derivative(y_true,z):\n",
    "  return (y_true-z)/(z)*(1-z)\n",
    "\n",
    "#Function to calculate the derivative of the sigmoid function\n",
    "def sigmoid_derivative(value):\n",
    "  return sigmoid(value) * (1-sigmoid(value))\n",
    "\n",
    "#Function to calculate the Binary Cross Entropy Loss\n",
    "def bce_loss(value,y_true):\n",
    "  return -1*((y_true * log(value)) + ((1-y_true) * log(1-value)))\n",
    "\n",
    "#Function to calculate the softmax\n",
    "def softmax(layer):\n",
    "  sum=0\n",
    "  for i in range(len(layer)):\n",
    "    sum+= 2.71828**layer[i]\n",
    "  for i in range(len(layer)):\n",
    "    layer[i]=2.71828**layer[i]/sum\n",
    "\n",
    "#Function to calculate categorical loss\n",
    "def categorical_loss(layer,y_true):\n",
    "  loss=log(layer[y_true])\n",
    "  return loss\n",
    "\n",
    "#Function to calculate derivative of categorical loss\n",
    "def cl_derivative(value,y_true, vocab_size=57):\n",
    "    #one_hot=[0,0,0]\n",
    "    one_hot=[0 for x in range(vocab_size)]\n",
    "    one_hot[y_true]=1\n",
    "    return value-one_hot[0]\n",
    "\n",
    "#Function to take deepcopy of an object\n",
    "def deep_copy(obj):\n",
    "    if isinstance(obj, list):\n",
    "        return [deep_copy(item) for item in obj]\n",
    "    elif isinstance(obj, dict):\n",
    "        return {key: deep_copy(value) for key, value in obj.items()}\n",
    "    else:\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "6d9695b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_LAYER = len(vocab)\n",
    "HIDDEN_LAYER = 3\n",
    "OUTPUT_LAYER = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "5f02d98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_neurons=[INPUT_LAYER, HIDDEN_LAYER, OUTPUT_LAYER]\n",
    "n=no_of_neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "e345110e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the weights here:\n",
    "Weights=[]\n",
    "for i in range(len(n)-1):\n",
    "  current=time.time()\n",
    "  m=int(str(current-int(current))[-1])\n",
    "  Weights.append([[(m/10 + x*random_gen()/100 - x*m*random_gen()/100+ random_gen())/10 for x in range(n[i+1])] for y in range(n[i])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "06f2f2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the biases here:\n",
    "Biases=[]\n",
    "for i in range(len(n)-1):\n",
    "  current=time.time()\n",
    "  m=int(str(current-int(current))[-1])\n",
    "  Biases.append([(m/10+x*random_gen()/100 - x*m*random_gen()/100+ random_gen())/10 for x in range(n[i+1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "bf051a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the gradients of weights here:\n",
    "Weights_Grad=[]\n",
    "for i in range(len(n)-1):\n",
    "  Weights_Grad.append([[0 for x in range(n[i+1])] for y in range(n[i])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "e772fa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the gradients of biases here:\n",
    "Biases_Grad=[]\n",
    "for i in range(len(n)-1):\n",
    "  Biases_Grad.append([0 for x in range(n[i+1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "2d0a06f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the activations here\n",
    "Activations=[]\n",
    "for i in range(len(n)-1):\n",
    "  Activations.append([0 for x in range(n[i+1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "ee98e45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define cached activations here\n",
    "ad=[]\n",
    "for i in range(len(n)-1):\n",
    "  ad.append([0 for x in range(n[i+1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "fefbdf66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.22000000000000003, 0.21800000000000003, 0.21600000000000003],\n",
       " [0.22000000000000003, 0.21800000000000003, 0.21600000000000003],\n",
       " [0.22000000000000003, 0.21800000000000003, 0.21600000000000003],\n",
       " [0.22000000000000003, 0.21800000000000003, 0.21600000000000003],\n",
       " [0.22000000000000003, 0.21800000000000003, 0.21600000000000003],\n",
       " [0.22000000000000003, 0.21800000000000003, 0.21600000000000003],\n",
       " [0.22000000000000003, 0.21800000000000003, 0.21600000000000003],\n",
       " [0.22000000000000003, 0.21800000000000003, 0.21600000000000003],\n",
       " [0.22000000000000003, 0.21800000000000003, 0.21600000000000003],\n",
       " [0.22000000000000003, 0.21800000000000003, 0.21600000000000003],\n",
       " [0.22000000000000003, 0.21800000000000003, 0.21600000000000003],\n",
       " [0.22000000000000003, 0.21800000000000003, 0.21600000000000003],\n",
       " [0.22000000000000003, 0.21800000000000003, 0.21600000000000003],\n",
       " [0.22000000000000003, 0.21800000000000003, 0.21600000000000003],\n",
       " [0.22000000000000003, 0.21800000000000003, 0.21600000000000003],\n",
       " [0.22000000000000003, 0.21800000000000003, 0.21600000000000003],\n",
       " [0.22000000000000003, 0.21800000000000003, 0.21600000000000003],\n",
       " [0.22000000000000003, 0.21800000000000003, 0.21600000000000003],\n",
       " [0.22000000000000003, 0.21800000000000003, 0.21600000000000003],\n",
       " [0.22000000000000003, 0.21800000000000003, 0.21600000000000003],\n",
       " [0.22000000000000003, 0.21800000000000003, 0.21600000000000003],\n",
       " [0.22000000000000003, 0.21800000000000003, 0.21600000000000003],\n",
       " [0.22000000000000003, 0.21800000000000003, 0.21600000000000003],\n",
       " [0.22000000000000003, 0.21800000000000003, 0.21600000000000003],\n",
       " [0.22000000000000003, 0.21800000000000003, 0.21600000000000003],\n",
       " [0.22000000000000003, 0.21800000000000003, 0.21600000000000003],\n",
       " [0.22000000000000003, 0.21800000000000003, 0.21600000000000003],\n",
       " [0.22000000000000003, 0.21800000000000003, 0.21600000000000003],\n",
       " [0.22000000000000003, 0.21800000000000003, 0.21600000000000003],\n",
       " [0.22000000000000003, 0.21800000000000003, 0.21600000000000003],\n",
       " [0.22000000000000003, 0.21800000000000003, 0.21600000000000003],\n",
       " [0.22000000000000003, 0.21800000000000003, 0.21600000000000003],\n",
       " [0.22000000000000003, 0.21800000000000003, 0.21600000000000003],\n",
       " [0.22000000000000003, 0.21800000000000003, 0.21600000000000003],\n",
       " [-0.8800000000000001, -0.8710000000000001, -0.8619999999999999],\n",
       " [-0.8800000000000001, -0.8710000000000001, -0.8619999999999999],\n",
       " [-0.8800000000000001, -0.8710000000000001, 0.386],\n",
       " [0.42000000000000004, 0.41600000000000004, 0.41200000000000003],\n",
       " [0.42000000000000004, 0.41600000000000004, 0.41200000000000003],\n",
       " [0.42000000000000004, 0.41600000000000004, 0.41200000000000003],\n",
       " [0.42000000000000004, 0.41600000000000004, 0.41200000000000003],\n",
       " [0.42000000000000004, 0.41600000000000004, 0.41200000000000003],\n",
       " [0.42000000000000004, 0.41600000000000004, 0.41200000000000003],\n",
       " [0.42000000000000004, 0.41600000000000004, 0.41200000000000003],\n",
       " [0.42000000000000004, 0.41600000000000004, 0.41200000000000003],\n",
       " [0.42000000000000004, 0.41600000000000004, 0.41200000000000003],\n",
       " [0.42000000000000004, 0.41600000000000004, 0.41200000000000003],\n",
       " [0.42000000000000004, 0.41600000000000004, 0.41200000000000003],\n",
       " [0.42000000000000004, 0.41600000000000004, 0.41200000000000003],\n",
       " [0.42000000000000004, 0.41600000000000004, 0.41200000000000003],\n",
       " [0.42000000000000004, 0.41600000000000004, 0.41200000000000003],\n",
       " [0.42000000000000004, 0.41600000000000004, 0.41200000000000003],\n",
       " [0.42000000000000004, 0.41600000000000004, 0.41200000000000003],\n",
       " [0.42000000000000004, 0.41600000000000004, 0.41200000000000003],\n",
       " [0.42000000000000004, 0.41600000000000004, 0.41200000000000003],\n",
       " [0.42000000000000004, 0.41600000000000004, 0.41200000000000003],\n",
       " [0.42000000000000004, 0.41600000000000004, 0.41200000000000003]]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Weights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "3109dd7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-0.63, -0.588, -0.546],\n",
       " [0.27,\n",
       "  0.258,\n",
       "  0.246,\n",
       "  0.23399999999999999,\n",
       "  0.22199999999999998,\n",
       "  0.21000000000000002,\n",
       "  0.198,\n",
       "  0.186,\n",
       "  0.174,\n",
       "  0.16199999999999998,\n",
       "  0.15,\n",
       "  0.13799999999999998,\n",
       "  0.126,\n",
       "  0.11399999999999999,\n",
       "  0.10200000000000001,\n",
       "  0.09,\n",
       "  0.07799999999999999,\n",
       "  0.06600000000000002,\n",
       "  0.054000000000000006,\n",
       "  0.041999999999999996,\n",
       "  0.030000000000000027,\n",
       "  0.017999999999999995,\n",
       "  0.005999999999999983,\n",
       "  -0.0060000000000000496,\n",
       "  -0.01799999999999997,\n",
       "  -0.02999999999999998,\n",
       "  -0.041999999999999996,\n",
       "  -0.054000000000000006,\n",
       "  -0.06600000000000002,\n",
       "  -0.07799999999999999,\n",
       "  -0.09000000000000004,\n",
       "  -0.10200000000000001,\n",
       "  -0.11400000000000006,\n",
       "  -0.12600000000000003,\n",
       "  -0.13799999999999998,\n",
       "  -0.15000000000000005,\n",
       "  -0.162,\n",
       "  -0.174,\n",
       "  -0.18600000000000003,\n",
       "  -0.198,\n",
       "  -0.20999999999999996,\n",
       "  -0.22200000000000006,\n",
       "  -0.23399999999999999,\n",
       "  -0.2459999999999999,\n",
       "  -0.258,\n",
       "  -0.2699999999999999,\n",
       "  -0.28200000000000003,\n",
       "  -0.29400000000000004,\n",
       "  -0.30599999999999994,\n",
       "  -0.31800000000000006,\n",
       "  -0.32999999999999996,\n",
       "  -0.34199999999999997,\n",
       "  -0.354,\n",
       "  -0.366,\n",
       "  -0.37799999999999995,\n",
       "  -0.39,\n",
       "  -0.40199999999999997]]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "e35ae971",
   "metadata": {},
   "outputs": [],
   "source": [
    "w=Weights\n",
    "b=Biases\n",
    "w_grad=Weights_Grad\n",
    "b_grad=Biases_Grad\n",
    "a=Activations\n",
    "#ca=Cached_Activations\n",
    "w_original=deep_copy(w)\n",
    "#b_original=deep_copy(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "3e9412b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(message,true_label,X):\n",
    "  input_layer=X[message]\n",
    "  for i in range(len(n)-2):\n",
    "    for j in range(n[i+1]):\n",
    "      for k in range(n[i]):\n",
    "        a[i][j]+=w[i][k][j] * input_layer[k]\n",
    "      a[i][j] += b[i][j]\n",
    "      a[i][j] = sigmoid(a[i][j])\n",
    "    input_layer = a[i]\n",
    "  softmax_layer=a[-2]\n",
    "  i=len(n)-2\n",
    "  for j in range(n[i+1]):\n",
    "    for k in range(n[i]):\n",
    "      a[i][j]+=w[i][k][j] * input_layer[k]\n",
    "    a[i][j]+=b[i][j]\n",
    "  output_layer=a[-1]\n",
    "  softmax(output_layer)\n",
    "  t=deep_copy(a)\n",
    "  loss=categorical_loss(output_layer,true_label)\n",
    "  for xx in range(len(a)):\n",
    "    for yy in range(len(a[xx])):\n",
    "      a[xx][yy]=0\n",
    "  return (loss,t)\n",
    "\n",
    "def get_loss(X_train,y_train):\n",
    "  net_loss=0\n",
    "  ca=[]\n",
    "  for message in range(len(X_train)):\n",
    "    loss,t=forward_pass(message,y_train[message],X_train)\n",
    "    net_loss+=loss\n",
    "    ca.append(t)\n",
    "\n",
    "  return (net_loss,ca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "002d7712",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ad(layer):\n",
    "  for i in range(len(w[layer])):\n",
    "    z=0\n",
    "    for message in range(len(X_train)):\n",
    "      z=ad[layer][0]*sigmoid_derivative(cache[message][layer][0])*w[layer][i][0]\n",
    "    z=z/len(X_train)\n",
    "    ad[layer-1][i]=z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "d5f7f677",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropogation(cache):\n",
    "  #Lets just update the last layer first\n",
    "  for number_cache in range(len(cache[0][-1])):\n",
    "    for number in range(len(w_grad[-1])):\n",
    "      grad_w=0\n",
    "      for message in range(len(X_train)):\n",
    "        grad_w+=(cl_derivative(cache[message][-1][number_cache], y_train[message]) * cache[message][-2][number])\n",
    "      grad_w=grad_w/len(X_train)\n",
    "      w_grad[-1][number][number_cache]=grad_w\n",
    "      w[-1][number][number_cache]-=(learning_rate*grad_w)\n",
    "  for number_cache in range(len(cache[0][-1])):\n",
    "    grad_b=0\n",
    "    for message in range(len(X_train)):\n",
    "      grad_b+=(cl_derivative(cache[message][-1][number_cache], y_train[message]))\n",
    "    grad_b=grad_b/len(X_train)\n",
    "    b_grad[-1][number_cache]=grad_b\n",
    "    b[-1][number_cache]-=(learning_rate*grad_b)\n",
    "  for f in range(len(w[-1])):\n",
    "    z=0\n",
    "    for message in range(len(X_train)):\n",
    "      z+=(cl_derivative(cache[message][-1][0],y_train[message])) *w[-1][f][0]\n",
    "    z=z/len(X_train)\n",
    "    ad[-2][f]=z\n",
    "  layer=-2\n",
    "  while(layer>=-1*(len(n)-1)):\n",
    "    for number_cache in range(len(cache[0][layer])):\n",
    "      for number in range(len(w_grad[layer])):\n",
    "        grad_w=0\n",
    "        for message in range(len(X_train)):\n",
    "          if layer==-1*(len(n)-1):\n",
    "            grad_w+=ad[layer][number_cache] * sigmoid_derivative(cache[message][layer][number_cache])*X_train[message][number]\n",
    "          else:\n",
    "            grad_w+=ad[layer][number_cache] * sigmoid_derivative(cache[message][layer][number_cache])*cache[message][layer-1][number]\n",
    "        grad_w=grad_w/len(X_train)\n",
    "        w_grad[layer][number][number_cache]=grad_w\n",
    "        w[layer][number][number_cache]-=(learning_rate*grad_w)\n",
    "    if layer==-1*(len(n)-1):\n",
    "      break\n",
    "    else:\n",
    "      calculate_ad(layer)\n",
    "    layer=layer-1\n",
    "  layer_b=-2\n",
    "  while(layer_b>=-1*(len(n)-1)):\n",
    "    for number_cache in range(len(cache[0][layer_b])):\n",
    "      grad_b2=0\n",
    "      for message in range(len(X_train)):\n",
    "        grad_b2+=ad[layer][number_cache] * sigmoid_derivative(cache[message][layer][number_cache])\n",
    "      grad_b2=grad_b2/len(X_train)\n",
    "      b_grad[layer_b][number_cache]=grad_b2\n",
    "      b[layer_b][number_cache]-=(learning_rate*grad_b2)\n",
    "    layer_b=layer_b-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "d46ade0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "80d701bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a markdown here explaining why y_train is the way I have made it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "1240bf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=[]\n",
    "X=[]\n",
    "y_train=[]\n",
    "for pair in dataset:\n",
    "    X_train.append(one_hot_encoding(pair[0].lower()))\n",
    "    y_train.append(one_hot_encoding(pair[1].lower()).index(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "2a219db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=[X_train[0]]\n",
    "y_train=[y_train[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4d3fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#But X_train is only the first element, and y_train is only the first element\n",
    "for i in range(1000):\n",
    "  l,cache=get_loss(X_train,y_train)\n",
    "  print(str(i)+\" : \" + str(l))\n",
    "  backpropogation(cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3973cf8d",
   "metadata": {},
   "source": [
    "#### This is the weight matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "1b82ab92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.21975602876603068, 0.21818486618719032, 0.21575914782873143],\n",
       " [0.21975602876603068, 0.21818486618719032, 0.21575914782873143],\n",
       " [0.21975602876603068, 0.21818486618719032, 0.21575914782873143],\n",
       " [0.21975602876603068, 0.21818486618719032, 0.21575914782873143],\n",
       " [0.2190240984098846, 0.21873945468937672, 0.21503657405168933],\n",
       " [0.21975602876603068, 0.21818486618719032, 0.21575914782873143],\n",
       " [0.21975602876603068, 0.21818475138098928, 0.21575914782873143],\n",
       " [0.21975602876603068, 0.21818475138098928, 0.21575914782873143],\n",
       " [0.21975602876603068, 0.21818475138098928, 0.21575914782873143],\n",
       " [0.21975602876603068, 0.21818475138098928, 0.21575914782873143],\n",
       " [0.21975602876603068, 0.21818475138098928, 0.21575914782873143],\n",
       " [0.21975602876603068, 0.21818475138098928, 0.21575914782873143],\n",
       " [0.21975602876603068, 0.21818475138098928, 0.21575914782873143],\n",
       " [0.21951205475603655, 0.21836950108762956, 0.21551829277996054],\n",
       " [0.21975602876603068, 0.21818475138098928, 0.21575914782873143],\n",
       " [0.21975602876603068, 0.21818475138098928, 0.21575914782873143],\n",
       " [0.21975602876603068, 0.21818475138098928, 0.21575914782873143],\n",
       " [0.21975602876603068, 0.21818475138098928, 0.21575914782873143],\n",
       " [0.21975602876603068, 0.21818475138098928, 0.21575914782873143],\n",
       " [0.22000000000000003, 0.21800000000000003, 0.21600000000000003],\n",
       " [0.21975602876603068, 0.21818475138098928, 0.21575914782873143],\n",
       " [0.2185361309653641, 0.21910848316666756, 0.21455484381873616],\n",
       " [0.21975602876603068, 0.21818475138098928, 0.21575914782873143],\n",
       " [0.21975602876603068, 0.21818475138098928, 0.21575914782873143],\n",
       " [0.21951205475603655, 0.21836950108762956, 0.21551829277996054],\n",
       " [0.21975602876603068, 0.21818475138098928, 0.21575914782873143],\n",
       " [0.21975602876603068, 0.21818475138098928, 0.21575914782873143],\n",
       " [0.21975602876603068, 0.21818475138098928, 0.21575914782873143],\n",
       " [0.21975602876603068, 0.21818475138098928, 0.21575914782873143],\n",
       " [0.21975602876603068, 0.21818475138098928, 0.21575914782873143],\n",
       " [0.21951205475603655, 0.21836950108762956, 0.21551829277996054],\n",
       " [0.21975602876603068, 0.21818475138098928, 0.21575914782873143],\n",
       " [0.21975602876603068, 0.21818475138098928, 0.21575914782873143],\n",
       " [0.21951205475603655, 0.21836950108762956, 0.21551829277996054],\n",
       " [-0.8802516497057196, -0.8708090812826907, -0.8622490346937337],\n",
       " [-0.880503300240182, -0.8706181630978077, -0.862498070327337],\n",
       " [-0.880503300240182, -0.8706181630978077, 0.3855225986133578],\n",
       " [0.4197584792246777, 0.4161828389205397, 0.41176164966443585],\n",
       " [0.41903389803280494, 0.41673134443228266, 0.4110465793935618],\n",
       " [0.41951695530476374, 0.41636567596621143, 0.41152329611793076],\n",
       " [0.4197584792246777, 0.4161828389205397, 0.41176164966443585],\n",
       " [0.4197584792246777, 0.4161828389205397, 0.41176164966443585],\n",
       " [0.4197584792246777, 0.4161828389205397, 0.41176164966443585],\n",
       " [0.4197584792246777, 0.4161828389205397, 0.41176164966443585],\n",
       " [0.41951695530476374, 0.41636567596621143, 0.41152329611793076],\n",
       " [0.4197584792246777, 0.4161828389205397, 0.41176164966443585],\n",
       " [0.4197584792246777, 0.4161828389205397, 0.41176164966443585],\n",
       " [0.4197584792246777, 0.4161828389205397, 0.41176164966443585],\n",
       " [0.4197584792246777, 0.4161828389205397, 0.41176164966443585],\n",
       " [0.41951695530476374, 0.41636567596621143, 0.41152329611793076],\n",
       " [0.41782619985139835, 0.4176454827755354, 0.4098547314168717],\n",
       " [0.4192754282406679, 0.41654851113684815, 0.4112849393608496],\n",
       " [0.4197584792246777, 0.4161828389205397, 0.41176164966443585],\n",
       " [0.41951695530476374, 0.41636567596621143, 0.41152329611793076],\n",
       " [0.4197584792246777, 0.4161828389205397, 0.41176164966443585],\n",
       " [0.4197584792246777, 0.4161828389205397, 0.41176164966443585],\n",
       " [0.4197584792246777, 0.4161828389205397, 0.41176164966443585]]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Weights[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc918956",
   "metadata": {},
   "source": [
    "##### Here are the embeddings of each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "7bb3c1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The embedding of this is [0.4192754282406679, 0.41654851113684815, 0.4112849393608496]\n",
      "The embedding of paper is [0.41951695530476374, 0.41636567596621143, 0.41152329611793076]\n",
      "The embedding of reviews is [0.4197584792246777, 0.4161828389205397, 0.41176164966443585]\n",
      "The embedding of the is [0.41782619985139835, 0.4176454827755354, 0.4098547314168717]\n",
      "The embedding of artificial is [0.21975602876603068, 0.21818486618719032, 0.21575914782873143]\n",
      "The embedding of intelligent is [0.21975602876603068, 0.21818475138098928, 0.21575914782873143]\n",
      "The embedding of algorithms is [0.21975602876603068, 0.21818486618719032, 0.21575914782873143]\n",
      "The embedding of in is [0.21951205475603655, 0.21836950108762956, 0.21551829277996054]\n",
      "The embedding of engine is [0.2185361309653641, 0.21910848316666756, 0.21455484381873616]\n",
      "The embedding of management is [-0.880503300240182, -0.8706181630978077, -0.862498070327337]\n",
      "The embedding of study is [0.4197584792246777, 0.4161828389205397, 0.41176164966443585]\n",
      "The embedding of provides is [0.41951695530476374, 0.41636567596621143, 0.41152329611793076]\n",
      "The embedding of a is [0.21975602876603068, 0.21818486618719032, 0.21575914782873143]\n",
      "The embedding of clear is [0.21975602876603068, 0.21818475138098928, 0.21575914782873143]\n",
      "The embedding of image is [0.21975602876603068, 0.21818475138098928, 0.21575914782873143]\n",
      "The embedding of of is [0.41903389803280494, 0.41673134443228266, 0.4110465793935618]\n",
      "The embedding of current is [0.21975602876603068, 0.21818475138098928, 0.21575914782873143]\n",
      "The embedding of state is [0.4197584792246777, 0.4161828389205397, 0.41176164966443585]\n",
      "The embedding of affairs is [0.21975602876603068, 0.21818486618719032, 0.21575914782873143]\n",
      "The embedding of for is [0.21951205475603655, 0.21836950108762956, 0.21551829277996054]\n",
      "The embedding of past is [0.4197584792246777, 0.4161828389205397, 0.41176164966443585]\n",
      "The embedding of 15 is [0.21975602876603068, 0.21818486618719032, 0.21575914782873143]\n",
      "The embedding of years is [0.4197584792246777, 0.4161828389205397, 0.41176164966443585]\n",
      "The embedding of and is [0.2190240984098846, 0.21873945468937672, 0.21503657405168933]\n",
      "The embedding of fresh is [0.21975602876603068, 0.21818475138098928, 0.21575914782873143]\n",
      "The embedding of insights is [0.21975602876603068, 0.21818475138098928, 0.21575914782873143]\n",
      "The embedding of improvements is [0.21975602876603068, 0.21818475138098928, 0.21575914782873143]\n",
      "The embedding of future is [0.21975602876603068, 0.21818475138098928, 0.21575914782873143]\n",
      "The embedding of directions is [0.21975602876603068, 0.21818475138098928, 0.21575914782873143]\n",
      "The embedding of field is [0.21975602876603068, 0.21818475138098928, 0.21575914782873143]\n",
      "The embedding of scope is [0.4197584792246777, 0.4161828389205397, 0.41176164966443585]\n",
      "The embedding of comprises is [0.21975602876603068, 0.21818475138098928, 0.21575914782873143]\n",
      "The embedding of three is [0.4197584792246777, 0.4161828389205397, 0.41176164966443585]\n",
      "The embedding of main is [-0.8802516497057196, -0.8708090812826907, -0.8622490346937337]\n",
      "The embedding of aspects is [0.21975602876603068, 0.21818475138098928, 0.21575914782873143]\n",
      "The embedding of to is [0.41951695530476374, 0.41636567596621143, 0.41152329611793076]\n",
      "The embedding of be is [0.21975602876603068, 0.21818475138098928, 0.21575914782873143]\n",
      "The embedding of discussed, is [0.21975602876603068, 0.21818475138098928, 0.21575914782873143]\n",
      "The embedding of namely, is [-0.880503300240182, -0.8706181630978077, 0.3855225986133578]\n",
      "The embedding of performance, is [0.4197584792246777, 0.4161828389205397, 0.41176164966443585]\n",
      "The embedding of control, is [0.21975602876603068, 0.21818475138098928, 0.21575914782873143]\n",
      "The embedding of diagnosis is [0.21975602876603068, 0.21818475138098928, 0.21575914782873143]\n",
      "The embedding of first is [0.21975602876603068, 0.21818475138098928, 0.21575914782873143]\n",
      "The embedding of is is [0.21951205475603655, 0.21836950108762956, 0.21551829277996054]\n",
      "The embedding of associated is [0.21975602876603068, 0.21818475138098928, 0.21575914782873143]\n",
      "The embedding of with is [0.4197584792246777, 0.4161828389205397, 0.41176164966443585]\n",
      "The embedding of need is [0.4197584792246777, 0.4161828389205397, 0.41176164966443585]\n",
      "The embedding of control is [0.21951205475603655, 0.21836950108762956, 0.21551829277996054]\n",
      "The embedding of basic is [0.21975602876603068, 0.21818475138098928, 0.21575914782873143]\n",
      "The embedding of characteristics is [0.21975602876603068, 0.21818475138098928, 0.21575914782873143]\n",
      "The embedding of that is [0.41951695530476374, 0.41636567596621143, 0.41152329611793076]\n",
      "The embedding of prove is [0.4197584792246777, 0.4161828389205397, 0.41176164966443585]\n",
      "The embedding of working is [0.4197584792246777, 0.4161828389205397, 0.41176164966443585]\n",
      "The embedding of properly, is [0.4197584792246777, 0.4161828389205397, 0.41176164966443585]\n",
      "The embedding of emission is [0.21975602876603068, 0.21818475138098928, 0.21575914782873143]\n",
      "The embedding of fuel is [0.21975602876603068, 0.21818475138098928, 0.21575914782873143]\n",
      "The embedding of economy is [0.22000000000000003, 0.21800000000000003, 0.21600000000000003]\n"
     ]
    }
   ],
   "source": [
    "word_set=set()\n",
    "for word in sentences:\n",
    "    if word not in word_set:\n",
    "        word_set.add(word)\n",
    "        print(f\"The embedding of {word} is {Weights[0][vocab[word.lower()]]}\")\n",
    "    else:\n",
    "        continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
